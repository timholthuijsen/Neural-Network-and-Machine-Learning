{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UnhealthyConversations.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyP7cBbDtT91dZiugGvQTvJ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"iZVWvof4Lnvn","colab_type":"code","colab":{}},"source":["!pip install tensorflow_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzBtUn1ELJZL","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","\n","import tensorflow as tf\n","import tensorflow_hub as tf_hub\n","import tensorflow_text as tf_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LdPakPp3LjqQ","colab_type":"code","colab":{}},"source":["SEQUENCE_LENGTH = 128\n","BATCH_SIZE = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7gF11KlSuV5","colab_type":"code","colab":{}},"source":["attributes = [\n","    'antagonize' , 'condescending', 'dismissive', 'generalisation',\n","    'generalisation_unfair', 'healthy', 'hostile', 'sarcastic']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"osT6J-NUMwQ6","colab_type":"text"},"source":["Build an model based on English"]},{"cell_type":"code","metadata":{"id":"zi5-A3PTLtd5","colab_type":"code","colab":{}},"source":["def model(batch_size, length, output_size, trainable_bert=True):\n","  \"\"\"Build and return a BERT model and tokenizer.\"\"\"\n","  inputs = {\n","      'word_ids': tf.keras.layers.Input(\n","          shape=(None,), dtype=tf.int32, name='word_ids'),\n","  }\n","  bert_layer = tf_hub.KerasLayer(\n","      'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2',\n","      trainable=trainable_bert)\n","  vocab_file = bert_layer.resolved_object.vocab_file.asset_path\n","  cased = bert_layer.resolved_object.do_lower_case\n","\n","  ids = inputs['word_ids']\n","  input_mask = tf.cast(tf.cast(ids, tf.bool), tf.int32)\n","  segment_ids = tf.zeros_like(ids, tf.int32)\n","\n","  pooled_output, _ = bert_layer([ids, input_mask, segment_ids])\n","  output = tf.keras.layers.Dense(batch_size, activation='tanh')(pooled_output)\n","  outputs = tf.keras.layers.Dense(\n","      output_size, activation='sigmoid', name='labels')(\n","          output)\n","\n","  return tf.keras.Model(inputs=inputs, outputs=outputs), vocab_file, cased"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8ypjT_1LxeL","colab_type":"code","colab":{}},"source":["bert, vocab_file, cased = model(\n","    batch_size=BATCH_SIZE, length=SEQUENCE_LENGTH, output_size=len(attributes))\n","bert.compile(\n","    loss=tf.keras.losses.BinaryCrossentropy(),\n","    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005),\n","    metrics=tf.keras.metrics.AUC(multi_label=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2g1tH6lvkxcs","colab_type":"code","colab":{}},"source":["tokenizer = tf_text.BertTokenizer(vocab_file, lower_case=cased)\n","# [CLS] and [SEP] token handling is not part of tf text http://b/160406014\n","cls, sep = tokenizer._wordpiece_tokenizer._vocab_lookup_table.lookup(\n","    tf.constant(['[CLS]', '[SEP]'])).numpy().tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqIxvMQnkmPL","colab_type":"text"},"source":["Preprocessing"]},{"cell_type":"code","metadata":{"id":"PkOMCM_KkiMA","colab_type":"code","colab":{}},"source":["def bert_feat(text, label=None):\n","  \"\"\"Maps text into the bert word_ids by tokenizing.\"\"\"\n","  rows = tf.size(text)\n","  tokens = tokenizer.tokenize(text).merge_dims(-2, -1)\n","  left = tf.fill((rows, 1), tf.cast(cls, dtype=tokens.dtype))\n","  right = tf.fill((rows, 1), tf.cast(sep, dtype=tokens.dtype))\n","  ids = {'word_ids': tf.concat([left, tokens, right], axis=1).to_tensor(\n","      0, shape=(None, SEQUENCE_LENGTH))}\n","  if label is not None:\n","    return (ids, {'labels': tf.cast(label, dtype=tf.float32)})\n","  return ids"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9c6w3B98Ms4n","colab_type":"text"},"source":["Load Data"]},{"cell_type":"code","metadata":{"id":"PgkPjYbPNgyU","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l12UIzVIL_YE","colab_type":"code","colab":{}},"source":["data = pd.read_csv('unhealthy_aggregated.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BqNayJGWN3rC","colab_type":"code","colab":{}},"source":["train = data.loc[data['_unit_id'] % 10 < 8]\n","dev = data.loc[data['_unit_id'] % 10 == 8]\n","test = data.loc[data['_unit_id'] % 10 == 9]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5oUqSe_TNu3X","colab_type":"code","colab":{}},"source":["train_dataset = tf.data.Dataset.from_tensor_slices(\n","    (train['comment'].astype(str), train[attributes]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4zbSyg9MwMHr","colab_type":"code","colab":{}},"source":["validation_dataset = tf.data.Dataset.from_tensor_slices(\n","    (dev['comment'].astype(str), dev[attributes])).batch(\n","        BATCH_SIZE, drop_remainder=True).map(bert_feat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNrVJN74RQFD","colab_type":"code","colab":{}},"source":["cached_train = train_dataset.repeat().shuffle(1024).batch(\n","        BATCH_SIZE, drop_remainder=True).map(bert_feat).prefetch(\n","            tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C64-04z2fc1U","colab_type":"code","colab":{}},"source":["steps_per_epoch = int(train.shape[0] / BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r40Ys7pqkaFi","colab_type":"code","colab":{}},"source":["history = bert.fit(\n","    cached_train, steps_per_epoch=steps_per_epoch, epochs=4,\n","    validation_data=validation_dataset, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zPb-tjxem3Dj","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"FIOVGtZbk6S6","colab_type":"code","colab":{}},"source":["test_dataset = tf.data.Dataset.from_tensor_slices(\n","    (test['comment'].astype(str),  test[attributes]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5FIaII5sRos","colab_type":"code","colab":{}},"source":["predictions = pd.DataFrame(bert.predict(\n","    test_dataset.batch(BATCH_SIZE).map(bert_feat)),\n","    columns=attributes, index=test.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rp2ppsI1sdmC","colab_type":"code","colab":{}},"source":["plt.figure()\n","for attribute in attributes:\n","  fpr, tpr, _ = metrics.roc_curve(\n","      test[attribute].astype(bool), predictions[attribute])\n","  auc = metrics.roc_auc_score(\n","      test[attribute].astype(bool), predictions[attribute])\n","  plt.plot(fpr, tpr, label='%s %g' % (attribute, auc))\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend(loc='lower right')\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eDHF9EHjttGO","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}